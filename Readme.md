
# ğŸŒ English to Urdu Translation Transformer

## ğŸ“Œ Overview

This project implements a **Transformer model** for translating English sentences into Urdu using **PyTorch**. It follows the architecture from the paper _â€œAttention is All You Needâ€_ by Vaswani et al., incorporating:

- Multi-head attention  
- Positional encoding  
- Feed-forward layers  

This repo contains:

- ğŸ§  Transformer architecture in `transformer.py`  
- ğŸ‹ï¸â€â™‚ï¸ Training logic in `train_transformer.py`  
- ğŸ“Š Dataset (not included): `english_to_urdu_dataset.xlsx`  

âš ï¸ **Note:**  
The current model uses only **one encoder and decoder layer** with a **limited dataset**, leading to **less accurate predictions**.

---

## ğŸ“ Project Structure

```
project-root/
â”‚
â”œâ”€â”€ transformer.py           # Core transformer model
â”œâ”€â”€ train_transformer.py     # Training pipeline
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ english_to_urdu_dataset.xlsx  # English-Urdu dataset (external)
```

---

## ğŸ§  `transformer.py` â€“ Model Architecture

### âœ¨ Components:

- **Utility Functions**
  - `get_device()`: Auto-selects CUDA or CPU
  - `scale_dot_product()`: Scaled attention with mask support

- **Positional Encoding**  
  Adds positional info using sin/cos functions.

- **Sentence Embedding**  
  Tokenizes, adds start/end tokens, applies positional encoding.

- **MultiheadAttention**  
  Parallel attention computation over multiple heads.

- **Encoder / Decoder Layers**  
  Includes self-attention, normalization, and feed-forward modules.

- **Transformer**  
  Final integration with encoder, decoder & linear output layer.

---

## ğŸ‹ï¸ `train_transformer.py` â€“ Training Script

### ğŸ§¹ Preprocessing:

- Loads `english_to_urdu_dataset.xlsx` using `pandas`
- Filters invalid sentences (e.g., >200 chars or unknown tokens)
- Builds vocabularies with `START`, `END`, and `PAD` tokens

### ğŸ“¦ Dataset:

- `TextDataset` for batching sentence pairs  
- `DataLoader` used for training

### ğŸ§  Masking:

- `create_masks()` generates:
  - Encoder padding mask
  - Look-ahead mask
  - Decoder cross-attention mask

### ğŸ” Training:

- **Hyperparameters**: `d_model=512`, `num_layers=1`, `num_heads=8`
- **Loss**: `CrossEntropyLoss` with padding ignored
- **Optimizer**: `Adam`
- **Loop**: Trains for 10 epochs  
- **Evaluation**: Translates sample: _"should we go to the mall?"_

---

## â— Known Issues

### ğŸ§¼ Unclean Dataset:
- Inconsistent tokenization
- Missing or noisy translations
- Invalid characters outside vocab

### ğŸ§  Limited Model Capacity:
- Only 1 encoder and decoder layer  
- Standard is 6â€“8 layers for translation tasks

### â±ï¸ Insufficient Training:
- 10 epochs not enough for convergence
- Learning rate may need tuning

### ğŸ“Š Small Dataset:
- Only 10,000 sentences used  
- More data = better generalization

---

## ğŸš€ Suggestions for Improvement

### ğŸ“ˆ Improve Dataset
- Clean punctuation, normalize Urdu diacritics
- Use larger corpora like OpenSubtitles

### ğŸ§± Expand Model
- Use 6â€“8 encoder/decoder layers
- Try `d_model=768+` or `num_heads=12`

### ğŸ§ª Tune Training
- Train for 50+ epochs
- Add learning rate scheduler + early stopping

### ğŸ”¬ Hyperparameter Tuning
- Try `lr=0.0005`, `drop_prob=0.3`, larger batch size

### ğŸ¯ Data Augmentation
- Back-translation or sentence paraphrasing

### ğŸ”  Tokenization
- Use **subword tokenization** (e.g., SentencePiece, BPE)

### ğŸ“ Evaluation
- Add **BLEU score** computation
- Use a proper validation set

### ğŸ§­ Inference
- Implement **beam search** instead of greedy decoding
- Fine-tune on domain-specific data

---

## ğŸ’» Installation & Usage

### ğŸ“¦ Prerequisites

```bash
pip install -r requirements.txt
```

**Requirements:**
- `torch>=2.0.0`
- `numpy>=1.24.0`
- `pandas>=2.0.0`
- `openpyxl>=3.1.0`

### ğŸ“ Dataset

- Place `english_to_urdu_dataset.xlsx` in the root directory  
- Or modify the path in `train_transformer.py`

### â–¶ï¸ Running the Model

```bash
python train_transformer.py
```

### ğŸ–¨ï¸ Output

- Filtered sentence count
- Epoch, iteration, and loss per batch
- Urdu translations of sample sentences

---

## ğŸ”® Future Work

- ğŸ§¹ Clean and expand dataset
- ğŸ§  Use deeper model architectures
- ğŸ“ Add BLEU and validation tracking
- ğŸŒ Deploy with a simple web interface

---

## ğŸ“Œ Notes

âš ï¸ The current model is experimental and has limitations due to:

- A single encoder/decoder layer
- Small, noisy dataset
- Basic training loop

For better results, use:

- A clean, larger dataset
- Deeper model (6â€“8 layers)
- Subword tokenization
